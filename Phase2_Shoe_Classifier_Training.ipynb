{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e2d6cae",
   "metadata": {},
   "source": [
    "# Phase 2: Shoe Type Specialist Classifier\n",
    "This notebook trains a specialized ResNet50 model to distinguish between different types of shoes.\n",
    "It uses the existing `fashion-dataset` but filters specifically for footwear categories.\n",
    "\n",
    "**Goal:** Provide the \"Intelligence\" for the \"Vibe Check\" logic (e.g., distinguishing Sneakers from Formal Shoes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cd28db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = r\"d:/AAI3001/fashion-dataset/fashion-dataset\"\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"styles.csv\")\n",
    "IMAGES_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "MODEL_SAVE_PATH = r\"d:/AAI3001/best_model_shoes.pth\"\n",
    "ENCODER_SAVE_PATH = r\"d:/AAI3001/le_shoes.pkl\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01c56eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Shoe Images Found: 9152\n",
      "articleType\n",
      "Casual Shoes    2845\n",
      "Sports Shoes    2036\n",
      "Heels           1323\n",
      "Flip Flops       914\n",
      "Sandals          897\n",
      "Formal Shoes     637\n",
      "Flats            500\n",
      "Name: count, dtype: int64\n",
      "Encoder saved to d:/AAI3001/le_shoes.pkl\n",
      "Classes: ['Casual Shoes' 'Flats' 'Flip Flops' 'Formal Shoes' 'Heels' 'Sandals'\n",
      " 'Sports Shoes']\n"
     ]
    }
   ],
   "source": [
    "# 1. Load and Filter Data\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH, on_bad_lines='skip')\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV: {e}\")\n",
    "    # Fallback for some pandas versions\n",
    "    df = pd.read_csv(CSV_PATH, error_bad_lines=False)\n",
    "\n",
    "# Define the Shoe Classes we care about\n",
    "SHOE_CLASSES = [\n",
    "    \"Casual Shoes\", \n",
    "    \"Sports Shoes\", \n",
    "    \"Formal Shoes\", \n",
    "    \"Heels\", \n",
    "    \"Flats\", \n",
    "    \"Sandals\", \n",
    "    \"Flip Flops\"\n",
    "]\n",
    "\n",
    "# Filter\n",
    "df_shoes = df[df[\"articleType\"].isin(SHOE_CLASSES)].copy()\n",
    "df_shoes[\"image_path\"] = df_shoes[\"id\"].apply(lambda x: os.path.join(IMAGES_DIR, f\"{x}.jpg\"))\n",
    "\n",
    "# Check file existence\n",
    "df_shoes = df_shoes[df_shoes[\"image_path\"].apply(os.path.exists)]\n",
    "\n",
    "print(f\"Total Shoe Images Found: {len(df_shoes)}\")\n",
    "print(df_shoes[\"articleType\"].value_counts())\n",
    "\n",
    "# Encode Labels\n",
    "le = LabelEncoder()\n",
    "df_shoes[\"label\"] = le.fit_transform(df_shoes[\"articleType\"])\n",
    "\n",
    "# Save Encoder\n",
    "with open(ENCODER_SAVE_PATH, \"wb\") as f:\n",
    "    pickle.dump(le, f)\n",
    "print(f\"Encoder saved to {ENCODER_SAVE_PATH}\")\n",
    "print(f\"Classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d787cca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 7321\n",
      "Val size: 1831\n"
     ]
    }
   ],
   "source": [
    "# 2. Dataset and DataLoader\n",
    "class ShoeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = row[\"image_path\"]\n",
    "        label = row[\"label\"]\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except:\n",
    "            # Fallback for corrupt images\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split\n",
    "train_df, val_df = train_test_split(df_shoes, test_size=0.2, stratify=df_shoes[\"label\"], random_state=42)\n",
    "\n",
    "train_dataset = ShoeDataset(train_df, transform=train_transform)\n",
    "val_dataset = ShoeDataset(val_df, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # num_workers=0 for Windows safety\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Val size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cf2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Setup\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze early layers (optional, but good for speed)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze layer4 for fine-tuning\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Replace Head\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_ftrs, len(le.classes_))\n",
    ")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c9a377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [06:05<00:00,  1.59s/it]\n",
      "Training: 100%|██████████| 229/229 [06:05<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7733 Acc: 0.7167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [01:23<00:00,  1.44s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.7065 Acc: 0.7346\n",
      "Saved new best model with acc: 0.7346\n",
      "Epoch 2/10\n",
      "Saved new best model with acc: 0.7346\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [02:58<00:00,  1.28it/s]\n",
      "Training: 100%|██████████| 229/229 [02:58<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5696 Acc: 0.7839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.42it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.5827 Acc: 0.7957\n",
      "Saved new best model with acc: 0.7957\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [02:57<00:00,  1.29it/s]\n",
      "Training: 100%|██████████| 229/229 [02:57<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5026 Acc: 0.8066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.43it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4708 Acc: 0.8225\n",
      "Saved new best model with acc: 0.8225\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [02:57<00:00,  1.29it/s]\n",
      "Training: 100%|██████████| 229/229 [02:57<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4001 Acc: 0.8466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [00:41<00:00,  1.41it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4514 Acc: 0.8236\n",
      "Saved new best model with acc: 0.8236\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [03:01<00:00,  1.26it/s]\n",
      "Training: 100%|██████████| 229/229 [03:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3733 Acc: 0.8547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.44it/s]\n",
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4641 Acc: 0.8209\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [02:55<00:00,  1.31it/s]\n",
      "Training: 100%|██████████| 229/229 [02:55<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3592 Acc: 0.8589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.43it/s]\n",
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4763 Acc: 0.8192\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [02:56<00:00,  1.30it/s]\n",
      "Training: 100%|██████████| 229/229 [02:56<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3391 Acc: 0.8661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.43it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4536 Acc: 0.8247\n",
      "Saved new best model with acc: 0.8247\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [02:55<00:00,  1.30it/s]\n",
      "Training: 100%|██████████| 229/229 [02:55<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3293 Acc: 0.8678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.42it/s]\n",
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4712 Acc: 0.8165\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [02:55<00:00,  1.30it/s]\n",
      "Training: 100%|██████████| 229/229 [02:55<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3321 Acc: 0.8701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.43it/s]\n",
      "Validation: 100%|██████████| 58/58 [00:40<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4525 Acc: 0.8236\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 229/229 [02:55<00:00,  1.31it/s]\n",
      "Training: 100%|██████████| 229/229 [02:55<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3298 Acc: 0.8697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 58/58 [00:39<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.4597 Acc: 0.8225\n",
      "Training Complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Training Loop\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    val_acc = correct / total\n",
    "    print(f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Saved new best model with acc: {best_acc:.4f}\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50b4a22",
   "metadata": {},
   "source": [
    "# Next Steps (The \"Eyes\")\n",
    "Now that you have the \"Brain\" (this classifier), you need the \"Eyes\" (Detector) to find the shoes in the first place.\n",
    "\n",
    "**To train the Detector (Step 1):**\n",
    "1. Download the **Fashionpedia** dataset (or a subset with 'shoe' labels).\n",
    "2. Train a YOLOv8 model on just the accessory classes.\n",
    "3. Use that YOLO model to crop the feet, then pass the crop to this ResNet model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
